# SPDX-FileCopyrightText: 2024 German Aerospace Center (DLR)
# SPDX-FileContributor: Stephan Druskat <stephan.druskat@dlr.de>
#
# SPDX-License-Identifier: CC0-1.0

# Main entrypoint of the workflow. 
# Please follow the best practices: 
# https://snakemake.readthedocs.io/en/stable/snakefiles/best_practices.html,
# in particular regarding the standardized folder structure mentioned there. 
import os


conda: local("envs/global.yaml")  # Make conda env accessible workflow-wide

configfile: local("config/config.yml")

WD = config["outdir"] if "outdir" in config else None
SUBSETS = config["subset"]
BASELINE_DATE = config["pmc_baseline_date"]

if WD is not None:
    workdir: WD

checkpoint bulk_download_pmc_metadata:
    input:
        storage.ftp(
            "ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_bulk/oa_noncomm/xml/oa_noncomm_xml.PMC"
            f"{{subset}}"
            "xxxxxx.baseline."
            f"{BASELINE_DATE}"
            ".tar.gz"
        ),
        storage.ftp(
            "ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_bulk/oa_comm/xml/oa_comm_xml.PMC"
            f"{{subset}}"
            "xxxxxx.baseline."
            f"{BASELINE_DATE}"
            ".tar.gz"
        ),
        storage.ftp(
            "ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_bulk/oa_other/xml/oa_other_xml.PMC"
            f"{{subset}}"
            "xxxxxx.baseline."
            f"{BASELINE_DATE}"
            ".tar.gz"
        )
    output:
        f"resources/metadata/oa_noncomm_xml.PMC{{subset}}xxxxxx.baseline.{BASELINE_DATE}.tar.gz",
        f"resources/metadata/oa_comm_xml.PMC{{subset}}xxxxxx.baseline.{BASELINE_DATE}.tar.gz",
        f"resources/metadata/oa_other_xml.PMC{{subset}}xxxxxx.baseline.{BASELINE_DATE}.tar.gz"
    threads: 1
    resources:
        runtime="2d"
    shell:
        "mkdir -p resources/metadata && cp {input} resources/metadata"

rule extract_archives:
    input:
        f"resources/metadata/oa_noncomm_xml.PMC{{subset}}xxxxxx.baseline.{BASELINE_DATE}.tar.gz",
        f"resources/metadata/oa_comm_xml.PMC{{subset}}xxxxxx.baseline.{BASELINE_DATE}.tar.gz",
        f"resources/metadata/oa_other_xml.PMC{{subset}}xxxxxx.baseline.{BASELINE_DATE}.tar.gz"
    output:
        directory("resources/metadata/PMC{subset}xxxxxx/")
    resources:
        runtime="6h"
    shell:
        "for FILE in {input}; do tar -xzvf $FILE -C resources/metadata; done"

rule write_luts:
    input:
        "resources/metadata/PMC{subset}xxxxxx/"
    output:
        "resources/luts/PMC{subset}.json"
    resources:
        runtime="6h"
    log:
        "logs/{subset}.log"
    script:
        "scripts/write_lut.py"


rule merge_luts:
    """
    Takes JSON files containing a dictionary and merges them into a single JSON file.
    """
    input:
        input=expand("resources/luts/PMC{subset}.json", subset=SUBSETS)
    output:
        "resources/pmc-publication-dates.json"
    resources:
        runtime="3h"
    script:
        "scripts/merge_luts.py"




checkpoint clone_git:
    """
    Clone the GitHub repository containing the source dataset 'Extract-URLs' locally.
    """
    output:
        git=directory("resources/extract-urls-git")
    shell:
        "git clone --depth 1 -b production --single-branch https://github.com/sdruskat/Extract-URLs.git {output}"


def _expand_parsed_pmc_url_files(wildcards):
    """
    Expands all files names for all JSON files in the directories
    {RESOURCES_DIR}pmc/<given resources_dir>/
    """
    git_dir = checkpoints.clone_git.get(**wildcards).output['git']
    file_names_pmc = glob_wildcards(os.path.join(git_dir,'pmc_parsed/{file_name}.json')).file_name

    return expand("resources/extract-urls-git/pmc_parsed/{file_name}.json",file_name=file_names_pmc)


rule patch_versions:
    """
    Checks whether all PMC IDs recorded in the parsed PMC part of the Extract-URLs dataset are in the LUTs
    and tries to retrieve manually missing versions.
    """
    input:
        lut=rules.merge_luts.output,
        pmc_urls=_expand_parsed_pmc_url_files,
    output:
        "resources/pmc-publication-dates.patched.json"
    threads: 1
    resources:
        runtime="3h"
    script:
        "scripts/patch_publications.py"


checkpoint split_luts:
    """
    Splits existing JSON LUT mapping PMC identifiers
    to publication dates by first digit of PMC identifier,
    and saves them into separate files. 
    """
    input:
        pathed_lut=rules.patch_versions.output
    output:
        outdir=directory("resources/split_luts/")
    resources:
        runtime="3h"
    script:
        "scripts/split_luts.py"

def _expand_split_luts(wildcards):
    split_luts_dir = checkpoints.split_luts.get(**wildcards).output[0]
    digit_lut = glob_wildcards(os.path.join(split_luts_dir,'PMC{digit}.json')).digit
    return expand(os.path.join(split_luts_dir, 'PMC{digit}.json'),digit=digit_lut)


rule all:
    """
    Produces an archive file containing lookup JSON files (PMC<zero-padded PMC prefix>.json) mapping
    PMC publication identifiers to publication dates in
    the format 'YYYY[-MM[-DD]]'. 
    """
    default_target: True
    input:
        _expand_split_luts
    output:
        "results/pmc-publication-dates-by-identifier.tar.gz"
    run:
        files = " ".join([fn.split("/")[-1] for fn in [str(fn) for fn in {input}][0].split(" ")])
        shell("tar -czvf {output} -C resources/luts {files}")


rule clean:
    threads: 1
    shell:
        "rm -rf resources/ results/ .cache/ .conda/ .snakemake/ logs/"  # Keeping extracted files
